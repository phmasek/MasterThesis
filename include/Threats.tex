\iffalse \bibliography{include/backmatter/magnus,include/backmatter/philip} \fi
\chapter{Threats to Validity}
In this section, the four perspective of validity threats presented by Runeson and H\"{o}st \cite{runeson} are discussed.
%\textit{Summary: In this section we will introduce potential weakenesses we are aware of by following the guidelines of Runeson \& HÃ¶st, Feldt \& Magazinius.}


\section{Construct Validity}
Construct validity refers to the degree of which the findings and observations made in the study reflect what is actually investigated in respect to the research questions. Data is collected in order to understand if the execution environments and deployment context impact the timing behaviour of the sample applications (experimental units). The sample applications are realised with the OpenDaVINCI library by using a skeleton code for time-triggered components from the tutorial section of OpenDaVINCI's online documentation. Minor modifications were made to the overall structure of the sample applications code base. This ensure that the sample applications used in the experiment is correct and in-line with the design requirements for real-time systems. OpenDaVINCI is open source and has been used to realise a number of autonomous vehicles. This adds validity to the use of the library. Furthermore, the sample application Pi/IO Component, used for capturing images from a web camera and storing them to disk is composed of code inherited from the OpenDLV open source project. The adds further validity to the correctness of the applications used in the experiment. The data captured from the sample applications are simply timestamps which fire in specified parts of the code base. Simple mathematics is applied to the set of measurement points to identify the duration. The duration is then compared over multiple runs in different execution and deployment contexts to uncover the impact.  
%------------------------%

\section{Internal Validity}
Internal validity refers to the extent to which an unknown factor has an effect on the factor under investigation or may refer to the issues that may affect the relationship between treatment and outcome. In order to mitigate the risk of an unknown factor interfering with the data being captured a number of strategies is used, namely: \\

\begin{enumerate}
\item{During the experimental run, all execution environments are initiated through a set of scenario scripts. Every initiated scenario is automated to ensure that no external factors may impact the outcome of the data during run time. Having each scenario automated further strengthens the assurance that when each scenario is re-initiated, the environment is a reproduction of the previous iteration of the same scenario. This mitigates any external factor interfering with the data being collected, however the scripts may include bugs not accounted for. To mitigate the risk of bugs manual validation runs were made with the intent to confirm that the execution environment is configured correctly as expected. Post validation was also carried out on inspecting the data collected which states the operating system configuration.}
\item{To mitigate any impact made on the performance on the system, the machine's network is disabled, as any traffic on the networking line may directly impact the operating system scheduler.}
\item{Data is captured through a serial port to bypass any performance impact made locally on the disk or through the USB kernel stack. The serial port is connected to an external device that handles the data capturing and processing it to a file. By writing all data through a serial port onto an external device any impact on the scheduler made by USB, memory, or disk is mitigated.}
\end{enumerate}
%------------------------%

\section{External Validity}
External validity refers to how generalisable are the findings beyond the actual study, and to what extent are the findings of interest and relevance outside of the scope. As already establish, scheduling precisions and I/O performance are two important factors for a CPS and to systems that are highly sensitive to timing delays. The results of this study can be generalizable for any system sensitive to timing delays in respect to the execution environment, the hardware and the specific versions of software used. The hardware used in the experiment is consumer level hardware, so reproduction of the experiment is easily viable and a relation to the results can easily be made. 
%------------------------%

\section{Conclusion Validity}
Conclusion validity refers to validity of the measured effect of the treatments used in the experiment. There exists two apparent threats to conclusion validity, namely Type I and Type II statistical errors. Due to the samples size of this research the threat of Type I error is considerable. For the statistical phenomena where an increasing sample size will result in a decreasing P-value. This research have thus put less weight on the P-value results as there exists a large number of data samples for the hypothesis testing \cite{kampenes2007systematic} which will result in highly significant results. This requires a more extensive analysis of the results, taking larger consideration on the effect size while evaluating the data.
%------------------------%







%something here about a incorrectly configured OS, kernel or a bug in that specific kernel version 

%---------------------------------%

% \begin{enumerate}
% \item{A script is used to execute the installation of all software packages requried for the experiment to run. All software packages that is installed on the target system is installed from an offline package manager. This ensure that all software packages installed on the system has equal versions when preparing the target system, and that precisely each step of the installation process is documented and available for review.}
% \item{The configuration file used to prepare the specific kernels used in this study are from reputable sources. These sources are from Ubuntu and the OSADL laboratory open source configuration files. This ensures that the kernels were configured in a correct and complete manner.}
% \item{A systematic checklist is used prior to executing the experiment. This ensures that the runtime configuration of the target system is documented and never altered until the experiment is completed.}
% \end{enumerate}

% dealing with operating systesm are complex beasts and so we can expect that random things happen, but we need a source for this.

%http://link.springer.com/article/10.1007%2Fs10664-008-9102-8


% Construct Validity
% \begin{enumerate}
% \item generlisable beyond the scope of the study, since the execution enviroment can be understood for different use cases. The hardware is typical COTS. 
% \end{enumerate}

% Conclusion Validity

% External Validity
% \begin{enumerate}
% \item generlisable beyond the scope of the study, since the execution enviroment can be understood for different use cases. The hardware is typical COTS. 
% \end{enumerate}

% How did we validate the results? 
	% statistical analysis is powerful 
	% according to the one paper "an updated performance.." running with no load results in no difference between. Our results are inline with those findings 
	% there exists papers that state using docker for IO should be taken with caution. How did our results perform? 
	% every run of tests is automated, perparing the target system is automated so we can ensure that results between eachother are the same. Ensures equal versioning between all enviroments
	% systematic checklist: ensures equal treatment across all enviroments

% How was the data actually validated?
	% we store data over serial line that uses a dedicated interrupt line so that storing the results does not have an effect on the experimental units
	% for the RT kernel, we ran the same test with our kernel and with Capocasa to see if there is a difference. Also, we use OSADL configuration file which is a organisation that benchmarks Linux RT kernel 

%clock monotonic


% Some papers say that operating systems are not good for measuring time, however, there are many other papers that use the same method as us but in a more simplistic way (without using any form of middleware (ODV))

% kernel configuration (did we do it right? - we did compare our results to Capocasa (a well usd RT Kernel) and there was no significant difference)

% Data comming from the RPI is valid since we know how many measurement points we expect 
% 


% every run of tests is automated, perparing the target system is automated so we can ensure that results between eachother are the same. Ensures equal versioning between all enviroments


% \begin{enumerate}
% \item{All conclusion drawn from the results are specific to the hardware and software used in the experiments. It has to be noted that the results may differ on other machines.}
% \item{The hardware is very specific for the case and cannot ensure the same results on either a personal computer or a high performing computer.}
% \end{enumerate}