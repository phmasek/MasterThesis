%Analyse <..> for the purpose of <..> with respect to their <..> from the point of view of the <..> in the context of <..>
\iffalse \bibliography{include/backmatter/magnus,include/backmatter/philip} \fi
\chapter{Research Methodology}\label{section:methodology}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\section{Experiment Planning}
% all information that is necessary to replicate the study 
In order to answer the research questions, an experiment is carried out following the guidelines for reporting experiments in software engineering \cite{Andreas}. All material for the experiment is available online\footnote{github.com/Pletron/ODVpi}. 


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Goals}
% refine the main research questions, describe and motivate them to allow for traceability down to the hypotheses. 
%Does the respective execution context influence the scheduling precision of the respective application?
%Does the respective execution context influence the input/output performance of the respective application?
The aim of the experiment is to systematically evaluate the scheduling precision and input-output performance of the experimental units running in different execution environments with respect to the deployment context. Understanding how the respective execution enviroment (e.g running the experimental untis in a Docker container with a real-time Linux kernel) influences sheduling precision and input-output performence will ultimately decide how deterministic - with respect to time - the system is said to be. Uncovering how deterministic the system is will answer the research questions. \\

The goals of the experiment is described in table~\ref{table:exp-goals}, where the experimental units, execution environment and system load is described further in this section. 

% execution environment: complete system 
% deployment context: docker non-docker 

\begin{table}[ht]
\begin{tabular}{l|p{12cm}}
Goal 1 & Analyse the scheduling precision of the Pi Component for the purpose of understanding how deterministic the system is with respect to the execution environment and system load. \\
Goal 2 & Analyse the input and output performance of the Pi/IO Component for the purpose of understanding how deterministic the system is with respect to the execution environment and system load.
\end{tabular}
\centering
\caption{Experiment Goals}
\label{table:exp-goals}
\end{table}


\subsection{Execution Environment} \label{section:exe-env}
The execution environment describes the setting in which the execution of the experimental units take place. Specifically, the execution environment refers to all components that make up a complete system: the processors, operating system, software packages and so on. There are four execution environments used in this experiment. The execution environments are configured the same but with two main differences. The difference being (1) an alternation of two Linux kernels and (2) an alternation of the deployment context. In this case, the deployment context is running the experiment units natively on the target system or running the experimental units in a Docker container. The execution environments are precisely specified in table~\ref{table:exe-env}, where the configuration of the execution environment is later described in section~\ref{section:exp-material}.


\begin{table}[ht]
\begin{tabular}{l|p{14cm}}
1. & Running the experimental units natively on the target system running a mainline vanilla Linux kernel.                               \\
2. & Running the experimental units natively on the target system running a mainline Linux kernel patched with RT\_Preempt.              \\
3. & Running the experimental units in a Docker container on the target system running a mainline vanilla Linux kernel.                  \\
4. & Running the experimental units in a Docker container on the target system running a mainline Linux kernel patched with RT\_Preempt.
\end{tabular}
\centering
\caption{Execution Environments}
\label{table:exe-env}
\end{table}


\subsection{System Load}
Submitting the target system to heavy load is required in order to (1) traverse as many code paths of the kernel as possible and (2) mimic the run-time load of a real-time system. Stress-ng \cite{stress-ng} is a user-space application that interacts with many components of the kernel. It can spawn a number of worker threads that perform useless tasks in order to apply load to the target system. The type of worker thread can vary from processor load to disk intensive load. Applying heavy load to the target system enables the experiment to be executed in an environment that mimics a real-life scenario. One would expect a real-time system to consume 80\% of the system's resources, leaving a 20\% buffer for extreme circumstances to circumvent system overload. \\

The experiment is performed in two scenarios: applying no-load and apply high-load to the system. Having these two scenarios allows for a fair comparison when the experimental units are executed in a more realistic operational enviroment. Having no load allows for the experimental units to be executed in isolated, controlled enviroment where a fair comparison can be done.

%Tracing through all possible code paths of the kernel is not an option to uncover how deterministic a system is. In order to get a reliable estimation, a tool is required to traverse as many of these as possible. Stress offers a way to induce several different kinds of system load, therefore it interacts with many components of the kernel and should be able to expose some of the longer code paths through the kernel. Stress is a user-space application that can spawn a number of worker threads that perform useless tasks with the only purpose of submitting the system on which it runs to heavy load. Putting the system under load also enables the experiment to be executed in an environment that mimics a real-life scenario. One would expect a computer system in production to run at 80\% load, leaving a 20\% buffer for extreme circumstances and to avoid overloading the system.  



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Experimental Units}
\label{section:exp-units}
% participants (experimental units) need to be described in detail. Sampling strategy, the number of participants, the kind of participants and the population they were drawn from. 

The experimental units, codenamed Pi and Pi/IO, are realised by the open source development library OpenDaVINCI. Using OpenDaVINCI as a development architecture for the experimental units allows the experiment to mimic the middle-ware one can expect in a autonomous vehicle. The implementation of OpenDaVINCI has been altered to enable full measurement of the system during runtime. The measurement points are simply timestamps of when the application reaches specific points in the code base during run-time. The specific changes that were made to the OpenDaVINCI library are: \\

\begin{enumerate}
\item Included specific measurement points that captures the current time within OpenDaVINCI's source code.
\item The measurement of time is modified to nanosecond precision.
\item A function is included to write measurement data to a serial port.\\
\end{enumerate}

%Further extensions were made to make sure the measurements were as precise as possible by enabling nanosecond precision. Lastly, Timestamp was altered to enable writing the data of the measurement points to a serial port, the code added can be found in the appendix \ref{code:serialtime}.The measurement points are simply timestamps of when the application reaches some points in the execution. An example of a measurement point within the Pi Component can be found in the appendix \ref{code:pi} at line \#3.\\

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Pi Component}
The Pi Component is utilised for measuring scheduling precision in order to answer RQ1. The Pi Component is tasked with calculating the next digit of Pi until it reaches 80\% of its' designated time-slice. The remaining 20\% of the time-slice is spent sleeping. The rationale for why it is specifically 80\% is for the ability to have a buffer that is reserved for sleeping. An alternative approach to the Pi Component was investigated. Namely, limiting the pi algorithm to only calculate \textit{x} digits of pi instead of calculating pi for \textit{x} amount of time. This approach made it difficult to predict whether the amount of pi digits calculated would occupy more than the specified timing requirement. The approach to limit the amount of pi calculations to 80\% was chosen as it provides a more controlled environment. The algorithm that calculates pi, specifically the Leibniz formula \cite{leibniz}, is merely there to simulate some load. In the actual development of a autonomous vehicle, the pi algorithm would be replaced with logic that is useful. The source code for the Pi Component can be found in Appendix~\ref{code:pi}, on page~\pageref{code:pi}.  


%The Pi Component is responsible for capturing data to measuring scheduling precision in order to answer RQ1. The Pi Component is tasked with calculating the next digit of Pi until it reaches 80\% of its' designated time-slice. The remaining 20\% of the time-slice is spent sleeping (\texttt{clock\_nanosleep}). The rationale of why it is specifically 80\% is due to the ability to have a buffer for sleeping to simulate a realistic scenario of code running for a particular time of the time-slice. An approach of limiting to a certain amount of pi-digits was investigated. However it would prove to be difficult to predict whether the amount of pi digits calculated would not occupy more than the specified time deadline. Therefore an approach to limit the pi calculation to a specific time of a time-slice was chosen as it provides a more controlled environment. The pi calculations are merely there to simulate some load and not to intrude on other components of the application, such as the overhead added by OpenDaVINCI and the accuracy of the system sleep. With the approach of limiting to pi-digits would be chosen, it may intrude on the actual overhead that the experiment sets out to measure.\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Pi/IO Component}
The Pi/IO Component is utilised for measuring input and output performance in order to answer RQ2. The component is an extension of the Pi Component with two additional factors. These factors are reading an image from a web camera (input) and storing the image to disk (output). Capturing the image and storing it will hereon be referred to as input performance \todo{Should it be camera performance? Should it be disk write performance?} and output performance respectively. The code implementation of capturing an image from a camera is a direct copy from the open-source project OpenDLV \cite{opendlv}. OpenDLV is open source software currently under development for the operation of an autonomous self-driving vehicle and is built upon OpenDaVINCI. Utilising source code from the OpenDLV project adds credibility to the experiment. Furthermore, the code implementation for storing an image to disk is also a direct copy from the OpenDLV project that is used for logging the system during runtime. In order to capture the specific performance measure of input and output, two measurement points are added to measure the overhead by capturing an image and storing it to disk. The source code for the Pi/IO Component can be found in Appendix~\ref{code:pi-io}, on page~\pageref{code:pi-io}.

%The Pi/IO Component is responsible for the measuring input/output performance in order to answer RQ2 (\ref{section:rqs}). The component extends the Pi Component by adding two additional factors to its time-slice execution. The factors are camera IO and disk IO. The code implementation of the camera component is a direct copy of code utilized by ReVeRe lab in a real-life scenario of a self-driving truck\footnote{https://github.com/chalmers-revere/opendlv}. This was copied to mimic real-life to ensure the relevance of the data captured. Furthermore, the code implementation for disk IO is also a direct copy used by ReVeRe lab for logging the system during runtime. Between these two additional factors, two measurement points are set to measure the overhead added by capturing an image and dumping that image to the disk.\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Experimental Material}
\label{section:exp-material}
% all experimental material and equipment should be described. all characteristics that might have an impact on the results should be mentioned here as formally as possible. 
In order for the experiment to begin, the target system needs to be prepared and configured with software. This section identifies the preparation of the target systems execution environment to ensure transparency and reproducibility of the experiment. This section introduces the hardware used in the experiment, the software environment and the runtime properties of the experimental units in respect to the deployment context. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  
\subsection{Hardware Environment}
The experiment presented in this report will be executed on a AEC 6950\footnote{http://www.aaeon.com/en/p/fanless-embedded-computers-aec-6950/} embedded personal computer manufactured by Aaeon. The AEC 6950 is an industrial computer marketed for data processing, machine control and fleet management applications \cite{aaeon}. The hardware specification for the target system can be found in table~\ref{table:hardware}. The hardware used to capture measurement data from the Aaeon AEC 6950 is a Raspberry Pi SoC \cite{raspberry} computer Model 1 B+. The data is collected from the target system via RS-232 serial communication by using a serial to USB converter.  

\begin{table}[ht]
\centering
\begin{tabular}{|l|l|}
\hline
\textbf{Component} & \textbf{Specification} 			\\ \hline
Processor          & Intel Core i7 3517UE 1.7 GHz 		\\ \hline
Memory             & 4GB DDR3 1333/1600 SODIMM          \\ \hline
Storage Device     & 2.5" SATA HDD x 1                  \\ \hline
Serial Interfaces  & \begin{tabular}[c]{@{}l@{}}USB type A x 2 for USB 2.0\\ USB type A x 2 for USB 3.0\\ DB-9 x 2 for RS-232/422/485 x 2\\ DB-9 x 4 for RS-232 x 4 \\ Isolated DB-9 x 2 for RS-232/422/485 x 2
\end{tabular} \\ \hline
\end{tabular}
\caption{Target System Hardware Specification}
\label{table:hardware}
\end{table}


\subsection{Software Environment} \label{soft-env}
The operating system used in the experiment is Ubuntu Server 14.04 Long Term Support. A long term supported version of Ubuntu Server is chosen due to its stability and balance between performance and advanced features. The Linux 3.18.25 kernel is selected for the operating system due a number of factors involving performance and software dependencies. Namely, Docker requires a kernel version greater than 3.10, limiting a large number of potential kernels. Secondly, Docker currently ships with \texttt{aufs} as the default back-end storage driver. However, the \texttt{aufs} storage driver has become depreciated in the Linux kernel and is therefore no longer available in current versions. Other options for the Docker storage-driver include \texttt{devicemapper}, \texttt{zfs}, \texttt{btrfs} and \texttt{overlayfs}. The \texttt{devicemapper} driver is known to have significant performance issues and the storage drivers \texttt{zfs} and \texttt{btrfs} currently have stability issues, so they are all disqualified. The remaining choice for the storage driver boils down to \texttt{overlayfs}, which requires a kernel version greater than 3.18. At the time of writing, the latest RT\_Preempt patch for kernel 3.18 which is actively maintained and supported is 3.18.25-rt23. Therefore, the RT\_Preempt patch 3.18.25-rt23 and kernel 3.18.25-generic is selected for this experiment. \\

All software packages that are required for the experiment are installed via the Linux package tool. The precise versions for each software package installed is documented and replicated when perparing the Docker images.The installation of software packages and subsequent configuration is executed via a script to document the changes that are made to the target system and to ensure precise reproducibility.

\subsubsection{Kernel Configuration} 
The vanilla Linux kernel 3.18.25-generic is downloaded from the official Ubuntu Kernel package archive \cite{kernel}. In order prepare the real-time enabled kernel, a specific set of kernel configurations are applied during the kernel build process. The default configuration file (\texttt{.config}) for the vanilla kernel is extracted and used as a basis for configuring the real-time enabled kernel. The modifications made to the default configuration file for the real-time enabled kernel follows the guidelines from \cite{rt-howto} and are specified in table~\ref{kernel-config}.  

\begin{table}[ht]
\begin{tabular}{|l|c|l|}
\hline
Parameter             		& \multicolumn{1}{l|}{Setting} & Description \\ \hline
CONFIG\_PREEMPT\_RT   		& \textit{y}                   & Enable preempt-rt patch.            \\ \hline
CONFIG\_LOCK\_DEBUGGING 	& \textit{n}                   & Disable schedule debugging information.            \\ \hline
HIGH\_RES\_TIMERS 			& \textit{y}				   & Enable high resolution timers. \\ \hline
ACPI\_DOCK					& \textit{n}				   & Disable ACPI management \\ \hline
ACPI\_PROCESSOR				& \textit{n}				   & Disable processor power management \\ \hline
\end{tabular}
\centering
\caption{Kernel Configuration Modifications}
\label{kernel-config}
\end{table}

\subsubsection{Docker Parameters}
The back-end manager for controlling Docker containers is performed by the \texttt{docker daemon}. The \texttt{docker daemon} is in charge of setting up the software environment for the containers as well as managing and controlling the container's resources. As mentioned in section~\ref{soft-env}, the \texttt{docker daemon} configuration file is changed to persistently use the \texttt{overlayfs} storage driver.\\

In order to build Docker images, a recipe in the form of a \texttt{DockerFile} is used when executing the \texttt{docker build} command line tool. The recipe contains instructions that construct the image, where each instruction that is executed is stored in a new subsequent storage layer. The relationship between a Docker image and container is similar to the relationship between a program and process. The runtime execution of a Docker image is a container, where any changes made in the container do not affect the parent image. Rather, changes that are made are stored within the container itself. The Docker images built for the experiment are all based on the official Ubuntu 14.04 LTS Docker image. The Ubuntu 14.04 LTS image is chosen to ensure a consistent environment between the native execution of the experimental units and that of the Docker containers. The software packages installed in the Docker images are all of equal versions to the packages that exist on the host. \\

To run a image the Docker \texttt{run} command is used which instantiates a container from the image. The experimental units, run from within a image, are executed with real-time scheduling priority 49. The runtime properties for containers that execute the experimental units are specified in table~\ref{docker-parameters}.

\begin{table}[ht]
\begin{tabular}{|l|p{10cm}|}
\hline
\textbf{Parameter}  		 & \textbf{Description}                                            \\ \hline
\texttt{-d}                  & Run the container in detached mode.                             \\ \hline
\texttt{--net=host}          & The networking configuration is derived from the host.          \\ \hline
\texttt{--cap-add=sys\_nice} & Allow access to devices such as the web camera and the serial port. \\ \hline
\texttt{-v}					 & Mount shared filesystems from the host into the container.	   \\ \hline
\texttt{-w}                  & The working directory to execute the experimental units.               \\ \hline
\end{tabular}
\centering
\caption{Docker Image Runtime Properties}
\label{docker-parameters}
\end{table}


\subsubsection{Native Execution Parameters} % is this a threat?
The native execution of the experimental units is performed by running executable binary files in a \texttt{screen} \cite{screen} session. The experimental units are executed with real-time scheduling priority 49. Linux screen is a full-screen window manager that, when called, creates a single window with a interactive shell session in it that can be used to execute programs. Multiple windows can be created, destroyed, detached and activated. Screen is used for the native execution for being able to run processes in a shell session that can be detached. 


\subsubsection{Stress Parameters}
When applying load to the target system, two processor worker threads are started that calculate pi with a set load of 80\%. Since the experimental units are run with real-time scheduling priority 49, the stress worker threads a set at priority 48 to ensure the experimental units are not pre-empted by the operating systems' scheduler. Starting the stress worker threads is always executed using \texttt{screen} shell session regardless of the deployment context of a particular experiment run-scenario. The precise execution of the \texttt{stress-ng} application is: \\

\begin{center}\fbox{\parbox{11cm}{\texttt{stress-ng --cpu 2 --cpu-load 80 --cpu-method pi --sched fifo --sched-prio 48}}}\end{center}



\section{Tasks}
The experimental units are executed in different execution environments with respect to the deployment context. The experimental units are tasked with capturing specified measurement points and sending the measurement data via serial communication to the data capturer where they are stored. A description of the experimental units is found in section \ref{section:exp-units} where the specific measurement points that are captured is found in section~\ref{section:dependent}.


\section{Variables and Hypotheses} %do we have any parameters?
Two types of variables are defined for the experiment: dependent and independent variables. The variables are analysed to evaluate the hypotheses of the experiment which are defined in this section. The experiment is run with an alternation between the treatment variables presented in section \ref{section:independent}. Table \ref{tab:scen-table} presents the four separate execution environments. Each of the execution environment are run with load and without load where running the application with load is the main source for analysis as it is of highest interest to understand how the execution environments behave when load is introduced.
%This section presents all hypotheses that will be analysed and answered in order to understand how the specified execution environments impact the performance of the experimental units.\\


\begin{table}[H]
\centering
\caption{Execution environment}
\label{tab:scen-table}
\begin{tabular}{|c|l|l|c|c|}
\hline
\textbf{scenario} & \textbf{deployment} & \textbf{kernel} & \parbox{2.3cm}{\centering\textbf{N with load}}& \parbox{2.3cm}{\centering\textbf{N without load}}	\\ \hline
1                 & native              & generic         &	5 					& 5							\\
2                 & native              & rt              &	5 					& 5							\\
3                 & docker              & generic         &	5 					& 5							\\
4                 & docker              & rt              &	5 					& 5							\\ \hline
\end{tabular}
\end{table}

%%%%%%%%%%%%%%%%%%%%%%
\subsection{Dependent Variables} \label{section:dependent}
%There exists groups of dependent variables which are all impacted by switching treatment. It is important to look both at the aggregated view of the variables, as well as at each of the variables independently. 

This research seeks to answer performance impact on 1) scheduling precision, 2) input performance, and 3) output performance. These three aggregated variables each consists of dependent variables which are impacted by the treatments presented in section \ref{section:independent}. Scheduling precision refers to how accurately, in terms of time, the system performs the necessary actions within a specified time-slice. The captured data points for measuring scheduling precision is the duration between four measurement points for the Pi Component. Raw data of these measurement points are the current time with a nanosecond accuracy, which is presented in \ref{table:measurement}. The location of the four measurement points within a time-slice is depicted in figure~\ref{pi_measure}, which represent the Pi Components complete time-slice. The duration between measurement points one and two, and between measurement points three and four reveal the overhead of the middle-ware OpenDaVINCI. The duration between measurement points four and one (the next time slice) reveals the sleep precision of the Pi Component with respect to the execution environment.\\ 

%Figure \ref{pi_measure} displays the four measurement points of the Pi Component time-slice, where the duration between measurement points \#1 and \#2, and between \#3 and \#4 reveals the necessary overhead of OpenDaVinci. While the duration between \#4 and \#1 (next time-slice) reveals the sleep precision of the execution environment.\\


\begin{table}[ht]
\begin{tabular}{|r|c|c|}
\hline
\textbf{Time-slice}  				&\textbf{Measurement point} 	& \textbf{Time-stamp}  \\ \hline
\multirow{4}{*}[-.1em]{Time-slice \#1}& 1							& 1463404523050041154  \\ \cline{2-3}
									& 2								& 1463404523050079895  \\ \cline{2-3}
									& 3								& 1463404523058096263  \\ \cline{2-3}
									& 4								& 1463404523058149089  \\ \hline
\multirow{4}{*}[-.1em]{Time-slice \#2}& 1							& 1463404523060041405  \\ \cline{2-3}
									& 2								& 1463404523060080287  \\ \cline{2-3}
									& 3								& 1463404523068095916  \\ \cline{2-3}
									& 4								& 1463404523068147008  \\ \hline
\end{tabular}
\centering
\caption{Raw data}
\label{table:measurement}
\end{table}

\begin{figure}[ht]
\centering
     \includegraphics[width=1.0\textwidth]{./figure/measurement_points_pi.png}
      \caption{Measurement points for Pi scheduling precision.}
       \label{pi_measure}
\end{figure}

The Pi/IO Component is an extension of the Pi Component with the addition of two measurement points added for measuring camera performance and disk write performance. The duration between measurement points two and three reveal the input performance of the Pi/IO component with respect to the execution environment. The duration between measurement points three and four reveals the output performance of saving an image to disk. The duration between measurement point two and four depict the total input-output performance of the executed time-slice. The remaining measurement points are the same as used in the Pi Component which are not analysed for answering research question 2.

%Figure \ref{piio_measure} displays the six measurement points of the Pi/IO Component time-slice, where the duration between measurement points \#2 and \#3 reveals the Camera IO performance of the execution environment. Duration between measurement points \#3 and \#4 reveals the Disk IO performance of the execution environment. Whereas the duration between \#2 and \#4 depicts the total IO performance of the executed time-slice. The remaining measurement points are the same points used for Pi Component.\\


\begin{figure}[ht]
\centering
     \includegraphics[width=1.0\textwidth]{./figure/measurement_points_piio.png}
      \caption{Measurement points for IO performance.}
       \label{piio_measure}
\end{figure}

 %\textit{Camera IO} refers to how much time is required for the application to capture an image from a camera. \textit{Disk IO} refers to how much time is required for the application to dump the image data to the disk. These three variables will be used separately to answer the two sets of hypotheses that addresses the two research goals. 

%%%%%%%%%%%%%%%%%%%%%%
\subsection{Independent Variables} \label{section:independent}
The treatments that will be used for assessing the impact are factors specific to the execution environment, namely: 1) the Linux kernel (a vanilla kernel or RT\_Preempt patched kernel), 2) the deployment context (Docker or native) and 3) system load (no load or high load). The research questions will be answered by analysing how each of the dependent variables are affected the independent variables during runtime. Each of the treatments are stored as binary figures in the data table (Table \ref{tbl:csv-structure}) where the true and false value is set depending on the scenario the measurement point is connected to. Table \ref{table:treat-alt} presents the binary alternation between the data scenarios

The control group is having each of the treatment variables set to $0$.

\begin{table}[ht]
\centering
\caption{Treatment alternation}
\label{table:treat-alt}
\renewcommand{\arraystretch}{1.2}
\begin{tabular}{|r|c|c|} \hline
							& \textbf{0}& \textbf{1}	\\ \hline
\textbf{Deployment Context}	& Native 	& Docker 		\\
\textbf{Kernel}				& Vanilla 	& RT\_preempt 	\\
\textbf{Load}				& No Load 	& CPU Load 		\\ \hline

\end{tabular}
\end{table}



%%%%%%%%%%%%%%%%%%%%%%
\subsection{Hypotheses}
The dependent variables are analysed to evaluate the hypotheses of the experiment. The alternative hypotheses are stated below, denoted $H_{1i_{j}}$. The \textit{i} corresponds to the goal identifier of the experiment where \textit{j} is a counter when more than one hypothesis is formulated per goal.\\

The deployment context is executing the experimental units on the target system natively or within a Docker container. The execution enviroment is an alternating of the independent variables. Further details on the execution enviroments are specified in \ref{section:exe-env}.

\begin{table}[H]
\begin{tabular}{l|l}
\multicolumn{2}{l}{The hypotheses to achieve goal one of the experiment are:} \\
$H_{11_{1}}$ & The deployment context has an impact on scheduling precision. \\
$H_{11_{2}}$ & The Linux kernel has an impact on scheduling precision. \\
$H_{11_{3}}$ & The execution environment has an impact on scheduling precision. \\          
\multicolumn{2}{l}{} \\
\multicolumn{2}{l}{The hypotheses to achieve goal two of the experiment are:} \\
$H_{12_{1}}$ & The deployment context has an impact on input performance.\\
$H_{12_{2}}$ & The Linux kernel has an impact on input performance.\\
$H_{12_{3}}$ & The execution environment has an impact on input performance.\\
$H_{12_{4}}$ & The deployment context has an impact on output performance.\\
$H_{12_{5}}$ & The Linux kernel has an impact on output performance.\\
$H_{12_{6}}$ & The execution environment has an impact on output performance.                                                          
\end{tabular}
\end{table}


%%%%%%%%%%%%%%%%%%%%%%
\section{Design}
In this controlled experiment a Quasi experiment design is used. A Quasi experiment design is used when a randomized experiment or when random assignment of study units to experimental groups is not possible \cite{kampenes}. Random assignment is not possible in this experiment since the experiment is a performance evaluation of the experiment units, being the Pi and Pi/IO components, with respect to the applied treatment. This study uncovers the cause and effect relationship between treatment and outcome. 

%%%%%%%%%%%%%%%%%%%%%%
\section{Procedure}% How will the experiment (i.e data collection) be performed? What instruments, materials, tools will be used and how
The overall procedure of the experiment is to execute the experimental units in 8 different scenarios. The scenarios consist of alternating two Linux kernels (vanilla and RT\_Preempt patched kernel), alternating two deployment contexts (executing in a Docker container or natively) and alternating between applying load and applying no-load to the target system. Executing the experiment run for each scenario is controlled from a  script. The script is started automatically when booting the system and is responsible for selecting a scenario, configuring the system for that particular scenario and executing the experimental unit. The experimental units are executed five times per scenario, totalling 80 runs for the entire experiment: \(2_{e} \times 2_{d} \times{} 2_{k}  \times{} 2_{l} \times{} 5 \) where $_{e}$ represents the two experiment units, $_{d}$ represents the deployment context, $_{k}$ represents the two kernels and $_{l}$ represents the two types of load. An overview of the procedure carried out by the execution script is depicted in figure~\ref{procedure-state}.


\begin{figure}[ht]
\begin{tikzpicture}[->,>=stealth',shorten >=1pt,auto,node distance=3cm,
                    semithick,initial distance=1cm,every initial by arrow/.style={*->}]
\tikzstyle{every state}=[fill=white,draw=black,text=black]
\node[initial,state] (A)                    {boot};
\node[state]         (B) [above right of=A] {wait};
\node[state]         (C) [below right of=B] {run};
  \path (A) edge [bend left]  node {} (B)
  		(A) edge [bend left]  node {\scriptsize id++} (B)
  		(B) edge [loop above] node {\scriptsize 60s} (B)
  		(C) edge [loop right] node {\scriptsize 3600s} (C)
  		(B) edge [bend left]  node {\scriptsize startScenario(id)} (C)
  		(C) edge [bend left]  node {\scriptsize switchKernel()} (A);
\end{tikzpicture}
\centering
\caption{State Machine Diagram of the Execution Script }
\label{procedure-state}
\end{figure}

	

%%%%%%%%%%%%%%%%%%%%%%
\section{Analysis Procedure}
The dependent variables are the duration between time measurement points within all executed time-slices using a measurement unit of \texttt{nanoseconds}. The procedure for processing the raw data captured from the experiment machine, is achieved using a python script\footnote{https://github.com/Pletron/[ADD REPO HERE]}. The script calculates the duration between two time-stamps using the equation shown in \ref{eq:calculate-duration} and stores the specific duration into a \textit{.csv} file together with the connected execution environment variables for the specific scenario run. The \textit{.csv} table structure and data types for each of the experimental units is presented in table \ref{tbl:csv-structure}.

\begin{equation} 
\label{eq:calculate-duration}
\begin{split}
duration = t_{n} - t_{n-1}
\end{split}
\end{equation}

\begin{table}[H]
\caption{CSV table structure for both experimental units}
\label{tbl:csv-structure}
\renewcommand{\arraystretch}{1.2}
\begin{tabular}{rL{2.9cm}|rL{2.9cm}}
\multicolumn{2}{c}{\textbf{Pi Component}}			& \multicolumn{2}{c}{\textbf{Pi/IO Component}}		\\ \hline
\textbf{Column names}	& \textbf{Measurement unit} & \textbf{Column names}	& \textbf{Measurement unit}	\\ \hline
deployment\_context		& binary					& deployment\_context		& binary				\\
kernel 					& binary					& kernel					& binary				\\
load 					& binary					& load						& binary				\\
overhead\_1				& nanoseconds				& overhead\_1				& nanoseconds			\\
pi\_calculation			& nanoseconds				& camera\_input				& nanoseconds			\\
overhead\_2				& nanoseconds				& disk\_output				& nanoseconds			\\
sleep 					& nanoseconds				& pi\_calculation			& nanoseconds			\\
						& 							& overhead\_2				& nanoseconds			\\
	 					& 							& sleep 					& nanoseconds			
\end{tabular}
\end{table}

Initially descriptive statistics are presented as a base for clarifying the variables used by statistical analyses for answering the hypotheses. Bar charts are constructed to present an overview of the result. This structure is the same between the two experimental units.\\

To address research question 1 a multivariate analysis (MANOVA) is conducted to understand the cause and effect relationship between the three variables: 1) deployment context; 2) kernel; 3) load, which construct the execution environment and the four measurement points collectively referred to as the scheduling precision. The MANOVA provides a P-values which are used to address the hypotheses connected to research question 1. The P-value explains if there exists a cause and effect relationship between the variables, however it does not disclose what that relationship is. To further understand what impact the treatments have on the dependent variables an $\eta^{2}$ value is calculated to reveal how large the impact is. Finally the cause and effect relationship is explained by analysing the coefficients between the treatments and dependent variables.\\

The second research question addresses two variables separately, namely input performance and output performance. Similar to the analysis procedure for research question 1, the second question seeks to understand the impact of the independent variables on the dependent variables. However the second research question have two separate dependent variables which are analysed separately to answer the first research question. Thus an ANOVA is conducted for each of the dependent variable to understand the impact between the treatment and independent variables. The ANOVA will reveal if there exists a cause effect relationship between the treatments on the dependent variables of input and output performance separately. Lastly the relationship is analysed through calculated $\eta^{2}$ values which will disclose how large the impact is.\\
