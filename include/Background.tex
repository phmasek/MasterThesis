\iffalse \bibliography{include/backmatter/magnus,include/backmatter/philip} \fi
\chapter{Background} \label{sect:background} 
This section introduces further background information on cyber-physical systems, real-time scheduling concepts, software deployment and container-based virtualization.
%---------------------------------------------------%
\section{Cyber Physical Systems}
%compact middle-ware OpenDaVINCI written in standard C++, can be used on a variety of POSIX OS. we use ODV to have a lean, portable and high-performance hardware and OS abstraction layer for typical programming idioms like concurrency, data storage and communication. %control, computing, communications 

A cyber physical system (CPS) interacts with the world, sensing their surroundings by using embedded sensors, actuators and processors. These sensors, atuators and processors communicate and collaborate with each other to support real-time, guaranteed performance in safety critical applications \cite{sidcyber}. Applications for CPSs span multiple domains including healthcare (robotic surgery), transportation (autonomous vehicles), agriculture, energy, and home automation to name a few. In the transportation domain for CPSs, autonomous vehicles, also called self-driving vehicles, are vehicles that operate without requiring human input. The intelligent behaviour of a self-driving vehicle include trajectory generation, lane following, lane keeping and intersection handling \cite{sidcyber}. These are some of the computational responsibilities of a CPS for driver-less vehicles.\\ 

The academic discipline of CPSs was formed in the 2000s \cite{alur} to help designers and developers deal with the complexity of realising CPSs \cite{gonz}. Even though forms of CPSs have been in industrial use for a long time, it is only recently that technology (such as wireless communication and processors) with impressive capabilities is available at low costs. This has led to the need for a more systematic approach to the development of reliable CPSs. Three main functional components of a CPS is control, computing and communications \cite{alur}.\\

One key factor for the functional component of computing is the need for real-time computations. Cyber-physical systems require real-time computations to ensure a high degree of safety when interacting with the world. Comparing a real-time system with an ordinary application, the difference is seen in how the execution of code is made. For an ordinary application an algorithm is executed once to provide a resulting output from an input without any specified timing constraints. However, a real-time system is recognised by its time constrained characteristic as the system is configured to execute an algorithm within a specified time-slice, i.e. executing an algorithm continuously which for each iteration takes \texttt{10 milliseconds}. This is a time-triggered component, that after the passing a specific amount of time, the component is once again executed. \\

Systems used for autonomous self-driving vehicles utilises a number of different algorithms to enable the self-driving functionality. Such algorithms may be responsible for processing a camera feed, detecting lanes within captured images or decision making such as steering, braking, or accelerating the vehicle depending on the content of input data. All these algorithms are embedded in a middle-ware application which sets the time constraints of the algorithms and handles the communication between them. This study utilises the open source middle-ware, OpenDaVINCI. OpenDaVINCI is compact middle-ware written in standard C++ that offers a high performing hardware and OS abstraction layers for concurrency, data storage and communication \cite{OpenDaVINCI}. OpenDaVINCI has been used to realise a number of self-driving vehicles.


\section{Real Time Scheduling}
%latency mitigated by modifying operating sytem to provide more determinism 
%linux foundation announced fully support rt linux 
%kernel and cpu scheduler
%http://site.ebrary.com/lib/chalmers/reader.action?docID=11042963&ppg=14


CPSs inherit the discipline of real-time systems requiring real-time computations so that the application exhibits the intended timing behaviour. A CPS is composed of multiple computer processes, each with their own demands for processing time. Orchestrating the demands of each process for processing time is the responsibility of the operating system scheduler. In this section we first introduce the concept of scheduling, describe scheduling precision and an introduce preemptive scheduling. 

\subsection{Scheduling Concepts}
A unit of computation that requires the allocation of processing time is referred to as a \textit{job}. Jobs can be event-driven (a specific event triggers activities in the system) or time-triggered (activities are initiated at predefined points in time). Time triggered events require strict timing behaviour and are necessary for hard real-time systems, thus being important for a CPS. When a time-triggered job is initiated, it communicates with the operating system scheduler to acquire processing time. The job enters a ready state assigned by the scheduler and starts processing. The job can then terminate (has reached the end of its processing) or become blocked which is what typically happens for time triggered jobs. A time-triggered job becomes blocked after processing the inner body of an algorithm, subsequently entering a sleep state to the scheduler. The job sleeps for a predefined amount of time: typically for how much time is left in the specified timing parameters. After the required time has elapsed, the job enters the state of being unblocked (waking the job after sleeping), hence ready to run for another execution cycle. A state machine of a scheduler, depicting all states is found in figure~\ref{scheduler}. In summary, a scheduler acts as a traffic police in a busy intersection, handling a queue of all the processes running on the system by prioritising some processes ahead of other processes. The scheduler may act and prioritise differently depending on what rules have been set for it to follow.\\         

\begin{figure}[ht]
\centering
\caption{Operating System Scheduler \cite{sched}.}
\label{scheduler}
\begin{tikzpicture}[->,>=stealth',shorten >=1pt,auto,node distance=3cm,
                    semithick,initial distance=1cm,every initial by arrow/.style={*->}]
\tikzstyle{every state}=[fill=white,draw=black,text=black]


\node[state,draw=none]	(x) [] 					{};
\node[state] 			(A) [right of=x]       {\small Ready};
\node[state]         	(B) [above right of=A] {\small Running};
\node[state]         	(C) [below right of=B] {\small Blocked};
\node[state,draw=none]	(D) [right of=B] {};
\node[elliptic state]         	(E) [below of=C,align=center] {\parbox{1.65cm}{\centering\small Blocked\\Suspended}};
\node[elliptic state]         	(F) [below of=A,align=center] {\parbox{1.65cm}{\centering\small Ready\\Suspended}};

\node[text width=2.3cm] (K) [above left of=A] {\scriptsize\parbox{2.3cm}{\textbf{(Sort Term)\\Scheduling}}};
\node[text width=2.3cm] (K) [below left of=A] {\scriptsize\parbox{2.3cm}{\textbf{Medium Term\\Scheduling}}};
\node[text width=9.3cm] (K) [below left of=E] {\scriptsize\parbox{9.3cm}{\textbf{Unblock is done by another task (a.k.a. wakeup, release, V)\\Block is a.k.a. sleep, request}}};

\path[->,label=below]			(x) edge []	node [] 	{\scriptsize Create} (A);
\path[->,label=top] 			(A) edge [bend right]	node [pos=0.5, sloped, above] 	{\scriptsize Run} (B);
\path[->,dotted,label=below]	(B) edge [bend right]	node [pos=0.5, sloped, below] 	{\scriptsize Prempt} (A);
\path[->,label=below]			(B) edge []	node [] 	{\scriptsize Block} (C);
\path[->,label=below]			(B) edge []	node [] 	{\scriptsize Terminate} (D);
\path[->,label=below]			(C) edge []	node [] 	{\scriptsize Unblock} (A);
\path[->,label=below]			(C) edge []	node [] 	{\scriptsize Suspend} (E);
\path[->,label=below]			(E) edge []	node [] 	{\scriptsize Resume} (C);
\path[->,label=below]			(E) edge []	node [] 	{\scriptsize Unblock} (F);
\path[->,label=below]			(F) edge []	node [] 	{\scriptsize Resume} (A);
\path[->,label=below]			(A) edge []	node [] 	{\scriptsize Suspend} (F);

\end{tikzpicture}
\end{figure}





\subsection{Scheduling Precision}
The experiment conducted in this study analyses the scheduling precision and input/output performance of the automotive real-time application. The application executes time-triggered computations within elements referred to as time-slices. The time-slice is a specification of time allocated for the algorithm to execute and deliver a result. A real-time application running at \texttt{100 hertz} executes 100 time-slices per second, which results in one time-slice being \texttt{10 milliseconds} or \texttt{0.01 second}. The \texttt{10 milliseconds} time-slice is the time deadline set for the specific application, which is the maximum time allowed for the assigned algorithm to finish its computations. In scenarios where the algorithm utilises less than the assigned time-slice the application will sleep for the remaining time until it fires a new execution. The operating system scheduler is responsible for assuring that the application sleeps for the time specified by the real-time application. The scheduler is also responsible for waking the real-time process after the specified sleeping time has elapsed. Other than the assigned algorithm, the real-time application consists of code which is responsible for communicating with the OS scheduler that controls the sleep of the time-slices. Therefore a part of the time-slice has to be consumed to execute the required code. 

%The time consumed by the code which controls the sleep of the application is referred to as the middle-ware overhead which is part of what is used for measuring the scheduling precision of the execution environments specified in the experiment of this study.\\

Scheduling precision refers to how accurately the application executes the specified algorithm from the point of firing the time-slice until the algorithm begins its computation. Further accuracy can be measured between the point of where the algorithm finishes its computations until the real-time application sleeps. Lastly, measurements can done to see whether the \texttt{sleep} function of the system actually sleeps for the remainder of the time-slice or if it overstays the specified timing deadline. It is important to understand how much time each part of the required code occupies the time-slice. The less time required for executing the code outside of the assigned algorithm the more deterministic a system is said to be. In a scenario where the assigned algorithm requires 80\% of the time-slice to execute the code, it is assumed that the application will sleep for the remaining 20\%. However, as there exists additional operations surrounding the algorithm the application might sleep 18\% whereas 2\% is required for the surrounding code to execute. If the application still sleeps 20\%, executes the algorithm for 80\%, and uses 2\% for the required code it will overstay its time-slice by 2\% thus rendering the application less deterministic (by a small amount). It is the time available for the algorithm the experiments of this study will seek to identify to inform software engineers of how much of the time-slice can be used for effective computations, i.e. time available for generating a result.

%The limitations of each execution environment utilised in the experiment of this study can be identified by understanding 

\subsection{Preemptive Scheduling}
A general purpose operating system scheduler implements a FIFO (first-in-first-out) approach with two process scheduler algorithms. Namely, a time-sharing algorithm and a real-time algorithm where the former is a scheduler that maintains fairness, distributing the system's resources equally over all processes in the queue ensuring that no process is completely starved. The later is an algorithm which prioritises the processes based on their set importance, where a higher prioritised process is provided more resources in comparison with a lower process. However, the generic Linux kernel version does not allow the scheduler to cancel all resources for processes already utilising the CPU. A higher prioritised process will therefore not be able to utilise 100\% of the CPU's resources if there are other processes already using the CPU. For a general purpose operating system this approach is standard practise and is logical for non time critical processes. However for a real-time system it is crucial to ensure that the highest level process can interrupt any running processes at any point in time and occupy all resources available to ensure the process meets the time deadline. A real time enabled kernel using the RT\_preempt kernel patch implements the preemptive approach thus allowing for such a behaviour of cancelling resources from lower prioritised processes to occupy all resources for a time-sensitive high priority process. Furthermore, the RT\_preempt patch applies resource locking for RT prioritised processes so that lower prioritised processes cannot utilise the same resource simultaneously.

%An operating system handles the communication between the software and the hardware, more specifically the operating system kernel acts as the interface between hardware and software. As an operating system is running a vast amount of processes simultaneously there is a need to prioritise and select which processes to run at what time. Each process utilises the CPU to make the computations required for the process to operate. The CPU is a powerful piece of hardware which can handle the same number of processes as it has cores and threads, i.e. an general Intel Core i7 processor has 4 cores and two threads per core amounting to 8 total processes simultaneously. However an operating system may run more processes than the CPU can process simultaneously. To manage this flood of processes a software component referred to as the \textit{CPU scheduler} which is configured by the kernel and is implemented to ensure that there is a queuing system set up for all processes running on the system. 

%---------------------------------------------------%
\section{Software Deployment}
%Software deployment for CPS is difficult as there are many aspects to put into consideration. Typically,  CPS are resource constrained, meaning that you cannot scale up hardware exponentially as you typically do not have the physical space for it. Secondly, real-time requirements are needed, so any SD tools needs to be lightweight. Finally, there are safety concerns since CPS typically interact with their surroundings. 

Software deployment is a crucial part of the software development, it refers to the activities which makes the software system available for use \cite{carzaniga1998characterization}. The process contains a number of activities which all play into the life cycle of a software system with the goal to implement into the runtime environment where the system is set to operate live. Carzaniga et al. describe the following main activities of software deployment: \\

\textbf{Release} – is the activity of packaging the software for delivering it to the end user. This includes processes such as including the software's requirements and dependencies to external components, such as libraries and applications. It also includes the process of advertising – the process of informing interested parties about the software being released.\\
\textbf{Install} – refers to the activities of assembling all required resources for the runtime environment. It consists of two specific process, namely \textit{transfer} and \textit{configuration}. Where the former is the process of transferring the software from the developer to the runtime environment and the later is the process of making the software ready for activation.\\
\textbf{Activate} – is the process of executing the software and all dependent applications in the runtime environment.\\
\textbf{Deactivate} – is the opposite of the \textit{activate} activity.\\
\textbf{Update} – is the activity of updating the version of the running software which consists of similar activities of the \textit{install} activity.\\
\textbf{Adapt} – refers to the process of ensuring that the updated version is running correctly in the runtime environment.\\
\textbf{Deinstall} – is the activity of decommissioning the running software and includes sub-activities such as removing the external libraries and components.\\
\textbf{Derelease} – is the final activity which includes the process of advertising the withdrawal of the software system.\\

All these activities differ in how they are executed depending on the software engineering paradigm utilised for the software project. Traditional software engineering practices, e.g. the waterfall model, seeks to execute the software deployment process at the end of the software's development cycle. Whereas more novel software engineering practices aim to execute the software deployment process continuously throughout the software's development cycle. In state-of-art software engineering practices such as continuous integration and continuous deployment, software is required to be deployed daily \cite{meyer2014continuous} for full adaptation. Such requirements can easily make the process of software deployment exhausting and complex, where software tools such as Docker would simplify these processes greatly. Docker simplifies processes found within each of the software deployment activities, as it provides the runtime environment before the software deployment process has begun. The development environment is a clone of the production environment thus transferring the deployment processes from the live production server to a confined secure location where the deployment process does not affect the usability of the current running software.\\


% \begin{figure}[ht]
% \centering
%       \caption{Run-time environment using Docker.}
%        \label{containers}
%      \includegraphics[width=1.0\textwidth]{./figure/containers.png}
% \end{figure}




\usetikzlibrary{arrows.meta}
\tikzset{>={Latex[width=2mm,length=2mm]}}
\tikzstyle{b} = [rectangle, draw, node distance=3cm, text width=6em, text centered, rounded corners, minimum height=4em, thick]
\begin{figure}[ht]
\caption{Run-time environment using Docker.}
\label{containers}



\begin{tikzpicture}
\tikzset{adjust fit placement}


\raggedleft
\begin{scope}[
		box/.style={%
			inner sep=0pt,
			align=center,
			text centered
		},scale=.9
	]
\node[box,draw,fit={(.3,-.5) (4.9,5.5)},fill=white,rounded corners] (perception) {};
\node[below] at (perception.north) (perceptiontext) {\small\textbf{Perception Node}};


\node[box,draw=black!90,fit={(1.4,1) (4.4,4)},fill=black!2] (v2per) {};
\node[below right] at (v2per.north west) () {\scriptsize{Docker}};
\node[below left] at (v2per.north east) () {\scriptsize\textbf{V2}};
\node[box,draw=black!70,fit={(1,.5) (4,3.5)},fill=black!2] (v1per) {};
\node[below right] at (v1per.north west) () {\scriptsize{Docker}};
\node[below left] at (v1per.north east) () {\scriptsize\textbf{V1}};
\node[draw,ellipse,dashed] at (v1per) () {\scriptsize Perception};

\node[box,draw,fit={(5.2,-.5) (9.8,5.5)},fill=white,rounded corners] (interpretation) {};
\node[below] at (interpretation.north) (interpretationtext) {\small\textbf{Interpretation Node}};

\node[box,draw=black!90,fit={(6.4,1.5) (9.4,4.5)},fill=black!2] (v3int) {};
\node[below right] at (v3int.north west) () {\scriptsize{Docker}};
\node[below left] at (v3int.north east) () {\scriptsize\textbf{V3}};
\node[box,draw=black!70,fit={(6,1) (9,4)},fill=black!2] (v2int) {};
\node[below right] at (v2int.north west) () {\scriptsize{Docker}};
\node[below left] at (v2int.north east) () {\scriptsize\textbf{V2}};
\node[box,draw=black!50,fit={(5.6,.5) (8.6,3.5)},fill=black!2] (v1int) {};
\node[below right] at (v1int.north west) () {\scriptsize{Docker}};
\node[below left] at (v1int.north east) () {\scriptsize\textbf{V1}};
\node[draw,ellipse,dashed,text width=1.7cm, minimum height=0.8cm,text centered,inner sep=0pt] at (v1int) () {\scriptsize Interpretation};


\node[box,draw,fit={(10.1,-.5) (14.7,5.5)},fill=white,rounded corners] (action) {};
\node[below] at (action.north) (actiontext) {\small\textbf{Action Node}};


\node[box,draw=black!90,fit={(11.2,1) (14.2,4)},fill=black!2] (v2act) {};
\node[below right] at (v2act.north west) () {\scriptsize{Docker}};
\node[below left] at (v2act.north east) () {\scriptsize\textbf{V2}};
\node[box,draw=black!70,fit={(10.8,.5) (13.8,3.5)},fill=black!2] (v1act) {};
\node[below right] at (v1act.north west) () {\scriptsize{Docker}};
\node[below left] at (v1act.north east) () {\scriptsize\textbf{V1}};
\node[draw,ellipse,dashed] at (v1act) () {\scriptsize Action};


\node[box,draw,fit={(0,0) (15,-1.2)},fill=black!2] (big) {\scriptsize Computational Nodes Cluster};



\draw[->] (v1per) -- (v1int);
\draw[->] (v2per.east) -| ++(0.7,1.2) -- ++(.9,0);
\draw[->] (v1per.east) ++(0,0.9) -| ++(0.7,1.35) -- ++(1.7,0);
\draw[->] (v1int) -- (v1act);
\draw[->] (v3int.east) ++(0,1.2) -| ++(0.5,-.4) -- ++(1.3,0);


\draw[->] (v2per.north) ++(-.7,0) |-  ++(-1.6,.4) |-  ++(0,-1.5) |- ++(.395,0) ;

\node[fit={(0.3,4.3) (2.4,5)}] {\scalebox{.6}{\Centerstack{A/B testing}}};
\end{scope}




\begin{scope}[line cap=round,line join=round,>=triangle 45,scale=0.2,xshift=-5cm,yshift=6.4cm]
\fill[line width=0.pt,fill=black,fill opacity=1.0] (0.,4.) -- (3.,4.) -- (3.,1.) -- (0.,1.) -- (0.,2.) -- (-1.,1.) -- (-1.,4.) -- (0.,3.) -- cycle node (camera) {};

\node[fit={(0,0) (7.6,3)},xshift=-.3cm,yshift=.95cm,align=left, inner sep=0.1ex] {\scalebox{.6}{\Centerstack{Camera\\Input Data}}};
\draw[->] (camera.east) ++(0,.5) -- (v1per.west);
\end{scope}


\end{tikzpicture}
\end{figure}



Figure \ref{containers} exemplifies a deployment strategy design utilising Docker in the context of self-driving vehicles. The responsibilities are broken down into three computational nodes in which Docker containers are running instances of the code base independently. Each Docker container can run different versions of the separate nodes, where the interpretation node has three versions running separately. Version one (denoted V1) in each node represents the latest working configuration while other versions are run to test code which is still under development. Having multiple versions of a running environment allows for safe and simple roll-backs in the event of buggy code or degraded performance. Furthermore, multiple versioning of the same Docker container allows for split testing between different containers to take place. With an always functioning configuration, the development team can demonstrate a functioning version of the product to stakeholders at any point in the development cycle. The ability to demonstrate the product at any point in the development phase adds to the business value of the project, as possible investors or stakeholders can be presented a functioning product although the product is currently under development. This is possible with current deployment approaches, however the implementation of cloning systems or running virtual machines is more resource demanding \cite{p6} and not as flexible in comparison with utilising Docker for the deployment strategy.


%---------------------------------------------------%
\section{Container-Based Virtualization}
%process isolation and application portability
%virtualization can be done with fully-fledged Virtual Machines or lightweight containers. VMs  incure higher ovhead but better protection and so are not suitable for autonomous vehicles.  Therefore we adopt containers and strive to minimize latency by real-time Linux kernel
%containers very efficiently share and utilize CPU and memory resources, 
% !!! Add in Dockerhub

Docker is an open source light-weight container manager that launched in 2013 and has gained ground rapidly with its simplicity. The environment offered by Docker simplifies the process of software deployment by packaging all dependencies into a light-weight virtual container which ensures that all instances of the software is utilising the same dependent libraries. The functionality provided by Docker is comparable with virtual machines as both are virtualised environments where software applications can be executed with all dependent libraries and applications pre-installed. However, Docker, in contrast to virtual machines, is a light-weight alternative as it communicates directly to the host machine's kernel. A virtual machine applies the additional levels of a virtualising an operating system which adds complexity since the virtual OS does not speak directly with the host machine's kernel, as depicted in figure~\ref{dockervm}. With the benefit of packaging the dependent libraries and applications into a container, software developers can avoid uncertain deployments where library versions may differ between the developers' development environments and the live production environment. Docker presents further benefits such as safe roll-back between different software versions which provides projects the ability to always be able to fall back on application versions which are known to function correctly. By providing these benefits project managers can feel secure in that there always exists a working runtime environment in the scenario of a new failing deployment.\\


\begin{figure}[ht]
\centering
\caption{Virtual Machines versus Docker \cite{whatdocker}}
       \label{dockervm}
\label{fig:vm-vs-docker-arch}
\begin{tikzpicture}[
		box/.style={%
			fit={(0,0) (1.8,.6)},
			inner sep=0pt,
			align=center,
			text centered
		}
	]
\tikzset{adjust fit placement}



\node[box,fit={(0,0) (1.8,.8)},fill=blues1] (app1) {\scriptsize App 1};
\node[box,fill=blues1] (app1lib) [below=0.1 of app1] {\scriptsize Bins/Libs};
\node[box,fit={ (0,0) (1.8,2)},fill=blues1] (app1os) [below=0.1 of app1lib] {\scriptsize Guest OS};


\node[box,fit={(0,0) (1.8,.8)},fill=blues2] (app2) [right=0.1 of app1] {\scriptsize App 2};
\node[box,fill=blues2] (app2lib) [below=0.1 of app2] {\scriptsize Bins/Libs};
\node[box,fit={ (0,0) (1.8,2)},fill=blues2] (app2os) [below=0.1 of app2lib] {\scriptsize Guest OS};

\node[box,fit={(0,0) (1.8,.8)},fill=blues3] (app3) [right=0.1 of app2] {\scriptsize App 3};
\node[box,fill=blues3] (app3lib) [below=0.1 of app3] {\scriptsize Bins/Libs};
\node[box,fit={ (0,0) (1.8,2)},fill=blues3] (app3os) [below=0.1 of app3lib] {\scriptsize Guest OS};

\node[box,fit={ (0,0) (5.635,.8)},fill=black!20] (hypervisor) [below=0.1 of app2os] {\scriptsize Hypervisor};
\node[box,fit={ (0,0) (5.635,.8)},fill=black!30] (vmhost) [below=0.1 of hypervisor] {\scriptsize Host Operating System};
\node[box,fit={ (0,0) (5.635,1.2)},fill=black!40] (vmhw) [below=0.1 of vmhost] {\scriptsize Computer Hardware};


\node[box,fit={ (0,0) (5.635,.8)},fill=black!50] (dockerengine) [right=1.6 of hypervisor] {\scriptsize Docker Engine};
\node[box,fit={ (0,0) (5.635,.8)},fill=black!30] (vmhost) [below=0.1 of dockerengine] {\scriptsize Host Operating System};
\node[box,fit={ (0,0) (5.635,1.2)},fill=black!40] (vmhw) [below=0.1 of vmhost] {\scriptsize Computer Hardware};

\node[box,fill=blues2] (dockerapp2lib) [above=0.1 of dockerengine] {\scriptsize Bins/Libs};
\node[box,fit={(0,0) (1.8,.8)},fill=blues2] (dockerapp2) [above=0.1 of dockerapp2lib] {\scriptsize App 2};

\node[box,fill=blues3] (dockerapp3lib) [right=0.1 of dockerapp2lib] {\scriptsize Bins/Libs};
\node[box,fit={(0,0) (1.8,.8)},fill=blues3] (dockerapp3) [above=0.1 of dockerapp3lib] {\scriptsize App 3};

\node[box,fill=blues1] (dockerapp1lib) [left=0.1 of dockerapp2lib] {\scriptsize Bins/Libs};
\node[box,fit={(0,0) (1.8,.8)},fill=blues1] (dockerapp1) [above=0.1 of dockerapp1lib] {\scriptsize App 1};



\node[box,fit={(0,0) (5.635,2)}] (dockerheading) [above=2.1 of dockerapp2] {\textbf{Docker Architecture}};
\node[box,fit={(0,0) (5.635,2)}] (vmheading) [above=0 of app2] {\textbf{Virtual Machine Architecture}};
\node[box,fit={(0,0) (5.635,1)}] (vmheading) [below=0 of vmhw] {};



\end{tikzpicture}
\end{figure}


The container in which Docker packages all dependent libraries and applications are referred to as a Docker image. This image contains everything which is required for an application to be executed. In the case of self-driving vehicles such dependencies may be image processing libraries such as OpenCV and the middle ware which enables the real-time application. When executing an application within Docker, a container is generated and executed based on the Docker image which consists of the installed libraries and applications. This software design allows for split testing of software as the same application can be executed multiple times without clashing with the other contained application. Thus being optimal for testing different versions of the same application simultaneously while knowing there is no interference between the executed applications.





%---------------------------------------------------%