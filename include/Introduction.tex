\iffalse \bibliography{include/backmatter/magnus,include/backmatter/philip} \fi
\chapter{Introduction}
In order for organisations to remain competitive, there is a need to continuously improve time to market for new features and services. The societal transformation of moving from a product economy to a service economy has affected the way organisations deliver software. Cloud computing is a direct response to the need of agility and has increasingly earned the reputation of being the holy grail of application deployment \cite{7034713}. Products are required to move from business requirements to delivery as fast as possible. What allows organisations to remain competitive by managing to release new services with such agility is a new software development methodology named DevOps. The aim of DevOps is to break the wall between developers and operations professionals in order to minimize deployment delays and to streamline the delivery process. New tools and new methodologies that help automate the entire release process are required in order to pursue a successful delivery process.\\

Virtual Machines (VM) and Linux Containers are popular tools that are used to simplify application release processes, especially in Cloud computing. Virtual machines permit workloads to be isolated from one another and for resource usage to be somewhat controlled \cite{7095802}. Docker, a Linux container manager, is different from running an entire virtual machine in that it delivers systems or applications packaged into containers. Instead of automating and making the deployment and management process as transparent and multi-platform as possible \cite{7095802}, Docker solves the same problem that faced the cargo industry; deliver goods in a standardized container so that any means of transport is capable of delivering the goods. A Dockerized application instance runs on a sort of lightweight VM with a complete copy of the file system, “sand-boxed” from other Dockerized instances. Each change to the file system in a container acts similarly to how revision control systems such as git works. Starting from a base image, any subsequent change is stored on a new layer which in turn decreases subsequent build times and allows for safe roll-backs.\\

When having a complete system decomposed in multiple containers, responsibilities are “sand-boxed” between each other during run-time, minimizing component dependencies. This allows development teams to work independently on each container with separate versioning. This avoids the big-bang integration approach and allows the possibility to carry out A/B testing for each separated component. Docker has been strongly adopted for web-applications, such as Twitter and Ebay \cite{7034713}. Having software components containerized of course introduces some performance overhead when compared to running the application natively, as identified in \cite{7034713}. The question remains, how much overhead to performance is introduced, when using Docker, especially when real-time requirements are mission critical?\\

Failing to meet the real-time requirements for autonomous self-driving vehicles could lead to a catastrophic effect. The software composed for self-driving vehicles could benefit by the use of virtualisation, during development (such as safe roll back and independent versioning) as well as post-development to ship updates and patches. In order to consider the use of visualisations for real-time systems, one needs to first understand the overhead that is introduced by virtualization.\\

\section{Background}

\section{Problem Domain \& Motivation}
Our current society is increasingly depending upon real-time systems and automated solutions for many of the fundamental building blocks to the modern world. Such as systems used within financial, aviation, and automotive sectors. Sectors where these systems play an integral role in enabling automation to provide the services we all depend on today. With these industries advancing rapidly there is a need in understanding how software engineers and data scientists should best work while developing the systems that drives the progress. With work, we specifically imply the way the work flow of these software projects is structured in terms of integration- and deployment strategies.\\

There exists no evidence which presents argumentation regarding the impact state-of-art deployment strategies have on real-time systems. This creates a compelling gap in literature to explore this field by building argumentation which decision makers can rely upon when determining for which strategy is suitable for the application in question. The popularity of deployment strategies utilizing containers is steadily increasing, thus making it intriguing to understand the performance overhead introduced by containers such as Docker. While the implementation of virtualization technologies for deployment strategies brings many advantages, there still exists uncertainty to the disadvantage of how much, if any, performance overhead they carry.\\

It is of particular importance to understand this impact for decision makers responsible for determining deployment strategies for real-time systems. The rationale being that real-time systems are time constrained and must guarantee responses within a specified deadline. If the system is to violate the specified deadline it may lead to software failure, which can potentially be catastrophic in the context of autonomous self-driving vehicles. Therefore it is crucial to ensure that the execution environment and deployment context will allow the real-time application to stay within its specified time deadline. This is the gap in which the result of this research will seek to fulfil. By gaining knowledge of whether Docker carry extensive performance overhead which will rule it out from the list of possible approaches used for software deployment in real-time systems.\\

\section{Research Goal \& Research Questions}
This research seeks to understand the performance impact Docker have on real-time systems in the context of autonomous self-driving vehicles, as this context provides a solid connection to a real life case. By analysing data extracted from executions made in an application which is being implemented in vehicles today, we seek to find firm evidence which considers overhead introduced by code required for executing the real-time application (RQ1). Additional overhead may exist when executing the real-time application with other load bearing factors such as writing and reading to disk, reading from a camera feed. Thus making it important to understand how Docker affects the performance of the real-time application when including external components which are required in real life (RQ2).\\

\begin{enumerate}[label=\textbf{RQ\arabic*}]
	\item How does the respective execution environment influence the scheduling precision of the respective application?
	\item How does the respective execution environment influence the input/output performance of the respective application?\\
\end{enumerate}





Two sets of runs for each RQ where 1 is without load and 1 is with load. 
- we want to find wether there is a correlation between enviroment and performance depending on load

If something is not clear here, move that to section Background

\section{Contributions}

\section{Scope}
Contribution is only applicable in these conditions

\section{Structure of the article}